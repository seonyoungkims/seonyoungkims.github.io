<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seonyoung Kim</title>
  
  <link rel="stylesheet" href="styles.css">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
</head>
<body>
  <main class="container">
    <!-- 이름 -->
    <h1 class="page-title">Seonyoung Kim</h1>

    <div class="page-links">
      <a href="mailto:seonyoungkim55@gmail.com">
        <img src="assets/images/email.svg" alt="" aria-hidden="true"> Email
      </a>
      <a href="assets/files/SeonyoungKim_CV.pdf" target="_blank">
        <img src="assets/images/cv.svg" alt="" aria-hidden="true"> CV
      </a>
<!--       <a href="https://scholar.google.com/citations?user=XXXX" target="_blank">
        <img src="assets/images/scholar.svg" alt="" aria-hidden="true"> Scholar
      </a> -->
<!--       <a href="https://github.com/yourusername" target="_blank">
        <img src="assets/images/github.svg" alt="" aria-hidden="true"> GitHub
      </a> -->
      <a href="https://www.linkedin.com/in/seonyoung-kim-b15037363/" target="_blank">
        <img src="assets/images/linkedin.svg" alt="" aria-hidden="true"> LinkedIn
      </a>
    </div>
  
    <!-- Hero -->
    <section class="hero">
      <div class="hero-left">
        <img src="assets/images/profile.jpeg" alt="Sunyoung Kim" class="avatar">
      </div>
      <div class="hero-right">
        <p>
          Hello, I am Seonyoung Kim, an AI Researcher in the SoC team at Samsung Research, where I focus on efficient inference of large-scale models and hardware-software (HW-SW) co-design. I have been interested in addressing the challenges of deploying models in resource-constrained systems. I have conducted research on the efficient deployment of LLMs, time-series models, and speech models deployment using various model compression techniques.
        </p>
        <p>
          My research focus lies in <strong>efficient ML/AI, efficient training, optimization, and hardware-aware AI design</strong>. Ultimately, I aim to develop highly efficient and widely accessible AI that can advance research and contribute to broader real-world applications.
        </p>
        <p>
          Previously, I received my B.S. in Computer Engineering from Hongik University, and my M.S. in Computer Science from Korea Advanced Institute of Science and Technology (KAIST), where I was advised by Professor Myoungho Kim.
        </p>
      </div>
    </section>
    
    <!-- Quick Links -->
    <section id="shortcuts" class="shortcuts">
      <h2>Contents</h2>
      <nav>
        <a href="#background">Research Background</a> |
        <a href="#publications">Publications</a> |
        <a href="#projects">Projects</a> |
        <a href="#talks">Talks & Presentations</a> |
        <a href="#honors">Honors & Scholarships</a> |
        <a href="#teaching">Teaching Assistant</a> |
        <a href="#activities">Extracurricular Activities</a>
      </nav>
    </section>


    <!-- Background -->
    <section id="background">
      <h2>Research Background</h2>
      <ul class="timeline">
        <li>
          <h3 class="timeline-header">
            AI Researcher | <a href="https://research.samsung.com" target="_blank" rel="noopener noreferrer">Samsung Research</a>
          </h3>
          <div class="header-period">
            Aug. 2022 – Present
          </div>
          <div class="timeline-body">
            <ul>
              <li><a href="https://research.samsung.com/data-intelligence" target="_blank" rel="noopener noreferrer"><a href="https://research.samsung.com/soc-architecture" target="_blank" rel="noopener noreferrer">SoC Architecture Team</a></li></li>
              <li>Focus: Model compression, LLMs, HW–SW co-design</li>
            </ul>
          </div>
        </li>
    
        <li>
          <h3 class="timeline-header">
            Research Assistant (RA) | <a href="https://www.kaist.ac.kr/en/" target="_blank" rel="noopener noreferrer">Korea Advanced Institute of Science and Technology (KAIST)</a>
          </h3>
          <div class="header-period">
            Sep. 2019 – Feb. 2022
          </div>
          <div class="timeline-body">
            <ul>
              <li><a href="http://dbserver.kaist.ac.kr/" target="_blank" rel="noopener noreferrer">Database Lab</a>, Advisor: Prof. Myoungho Kim</li>
              <li>Focus: Knowledge Distillation, Time-series anomaly detection</li>
            </ul>
          </div>
        </li>
    
        <li>
          <h3 class="timeline-header">
            Undergraduate Researcher | <a href="https://www.hongik.ac.kr/en/index.do" target="_blank" rel="noopener noreferrer">Hongik University</a>
          </h3>
          <div class="header-period">
            Nov. 2018 – Aug. 2019
          </div>
          <div class="timeline-body">
            <ul>
              <li><a href="https://apl.hongik.ac.kr/home" target="_blank" rel="noopener noreferrer">Research Lab for Distributed INtelligence and Autonomy (DINA)</a>, Advisor: Prof. Young Yoon</li>
              <li>Focus: AI, Data analysis, Distributed system </li>
            </ul>
          </div>
        </li>
      </ul>
    </section>
    
    <!-- Publications -->
    <section id="publications"><h2>Publications</h2>
      <ul class="list">        
        <li class="media-item">
          <div class="media-info">
            <h3 class="media-title">
              <a href="assets/papers/Bespoke_LUT__Non_Linear_Approximation_for_Integer_only_Transformer_Inference_on_NPUs.pdf" target="_blank">
                Bespoke LUT: Non-Linear Approximation for Integer-only Transformer Inference on NPUs
              </a>
            </h3>
        
            <p class="media-authors">
              <strong>Seonyoung Kim*</strong>, Jooeun Kim*, Hayoung Yun, Meejeong Park, Sangjeong Lee, Hanjoo Cho, Heonjae Ha
            </p>
        
            <p class="media-meta"><i>Under Review at IEEE Transactions on Emerging Topics in Computing</i>, 2025</p>
        
<!--             <p class="media-paper">
              <a href="assets/papers/P2.pdf" target="_blank">Paper</a>
            </p> -->
        
            <!-- ★ 그림을 Paper 뒤, 설명 앞에 둔다 -->
            <img src="assets/images/publication3.svg" alt="Overview figure" class="media-thumb">
        
            <p class="media-desc">
              Bespoke LUT uses per-layer dual-range lookup tables to approximate nonlinear operations on NPUs, achieving up to 3.3× speedup with minimal accuracy loss.
            </p>
          </div>
        </li>

        <li class="media-item">
          <div class="media-info">
            <h3 class="media-title">
              <a href="https://openreview.net/forum?id=MPMyROJvJV" target="_blank">
                RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs
              </a>
            </h3>

            <p class="media-authors">
              Youngcheon Yoo, Banseok Lee, Minseop Choi, <strong>Seonyoung Kim</strong>, Hyochan Chong, Changdong Kim, Dongkyu Kim, Youngmin Kim
            </p>
        
            <p class="media-meta"><i>Under Review at ICLR 2026</i></p>
        
<!--             <p class="media-paper">
              <a href="assets/papers/P2.pdf" target="_blank">Paper</a>
            </p> -->
        
            <!-- ★ 그림을 Paper 뒤, 설명 앞에 둔다 -->
            <img src="assets/images/publication4.png" alt="Overview figure" class="media-thumb">
        
            <p class="media-desc">
              ReBiT is a residual binarization framework for LLMs that achieves state-of-the-art 2-bit accuracy and up to 4.49× faster inference on Llama while halving optimizer memory.
            </p>
          </div>
        </li>
        
        <li class="media-item">
          <div class="media-info">
            <h3 class="media-title">
              <a href="assets/papers/Generating_Small_Anomaly_Detection_Models_through_Distillation_of_Long_Term_Dependency.pdf" target="_blank">
                Generating Small Anomaly Detection Models through Distillation of Long-Term Dependency
              </a>
            </h3>
        
            <p class="media-authors">
              <strong>Seonyoung Kim</strong>
            </p>
        
            <p class="media-meta"><i>Master Thesis</i>, 2022</p>
        
<!--             <p class="media-paper">
              <a href="assets/papers/Kim2022_Thesis_AnomalyDetection.pdf" target="_blank">Paper</a>
            </p> -->
        
            <!-- ★ 그림을 Paper 뒤, 설명 앞에 둔다 -->
            <img src="assets/images/publication2.png" alt="Overview figure" class="media-thumb">
        
            <p class="media-desc">
              Lightweight attention-based model with knowledge distillation and Long-Term Accumulator achieves 95% parameter reduction and 50% faster inference while preserving accuracy in time-series anomaly detection.
            </p>
          </div>
        </li>

        
        <li class="media-item">
          <div class="media-info">
            <h3 class="media-title">
              <a href="assets/papers/Knowledge_Distillation_for_Anomaly_Detection_in_Multivariate_Time_Series_Data.pdf" target="_blank">
                Knowledge Distillation for Anomaly Detection in Multivariate Time Series Data
              </a>
            </h3>
        
            <p class="media-authors">
              <strong>Seonyoung Kim</strong>, Myoungho Kim
            </p>
        
            <p class="media-meta"><i>KCC Oral</i>, 2021</p>
        
<!--             <p class="media-paper">
              <a href="assets/papers/Kim2021_KCC_AnomalyDetection.pdf" target="_blank">Paper</a>
            </p> -->
        
            <img src="assets/images/publication1.png" alt="Overview figure" class="media-thumb">
        
            <p class="media-desc">
              Developed an LSTM-based knowledge distillation method for multivariate time-series anomaly detection, reducing parameters by ~60% while maintaining accuracy.
            </p>
          </div>
        </li>

      </ul>
    </section>

    
    <!-- Projects -->
    <section id="projects"><h2>Projects</h2>
      <ul class="list">
    
        <!-- <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr6.png" alt="Efficient AI Deployment on NPUs thumbnail" class="project-thumb">
            <h3 class="project-title">Efficient AI Deployment on NPUs</h3>
            <p class="project-period">Aug. 2022 – Present</p>
            <p class="project-desc">
              At Samsung Research, I research hardware-aware model compression techniques, such as ultra low-bit quantization, to enable efficient deployment on NPUs. My work spans large-scale models including LLaMA, Gemma, and speech models.
            </p>
          </div>
        </li> -->
        <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr6.png" alt="Efficient AI Deployment on NPUs thumbnail" class="project-thumb">
            
            <h3 class="project-title">
              <a href="pages/projects/project-npu.html">Efficient AI Deployment on NPUs</a>
            </h3>
            
            <p class="project-period">Aug. 2022 – Present</p>
            <p class="project-desc">
              At Samsung Research, I research hardware-aware model compression techniques, such as ultra low-bit quantization, to enable efficient deployment on NPUs. My work spans large-scale models including LLaMA, Gemma, and speech models.
            </p>
          </div>
        </li>
    
        <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr4.png" alt="Edge Computing–Based Anomaly Detection in Memory Semiconductor Processes thumbnail" class="project-thumb">
            <!-- <h3 class="project-title">Edge Computing–Based Anomaly Detection in Memory Semiconductor Processes</h3> -->
            <h3 class="project-title">
              <a href="pages/projects/project-edgeComputing.html">Edge Computing–Based Anomaly Detection in Memory Semiconductor Processes</a>
            </h3>
            <p class="project-period">Sep. 2020 – Sep. 2021</p>
            <p class="project-desc">
              Developed an autoencoder–LSTM system for anomaly detection and remaining useful life (RUL) prediction, enabling predictive maintenance and improved process efficiency.
            </p>
          </div>
        </li>
    
        <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr3.png" alt="CMP Wafer Defect Detection Project thumbnail" class="project-thumb">
            <!-- <h3 class="project-title">Chemical Mechanical Planarization (CMP) Wafer Defect Detection Project</h3> -->
            <h3 class="project-title">
              <a href="pages/projects/project-cmp.html">Chemical Mechanical Planarization (CMP) Wafer Defect Detection Project</a>
            </h3>
            <p class="project-period">Dec. 2019 – Jun. 2020</p>
            <p class="project-desc">
              Built a deep learning model to classify wafer states and visualize defect regions with class activation maps, enhanced by generative models for virtual defect wafer images.
            </p>
          </div>
        </li>
    
        <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr2.png" alt="Neouly Security Project thumbnail" class="project-thumb">
            <!-- <h3 class="project-title">Neouly Security Project</h3> -->
            <h3 class="project-title">
              <a href="pages/projects/project-neouly.html">Neouly Security Project</a>
            </h3>
            <p class="project-period">Dec. 2018 – Jun. 2019</p>
            <p class="project-desc">
              Developed a malware detection model using Cuckoo Sandbox and a Deep Neural Network, achieving 97.8% accuracy and surpassing the top team in the KISA Data Challenge.
            </p>
          </div>
        </li>
    
        <li class="project-item">
          <div class="project-info">
            <img src="assets/images/Pr1.png" alt="AI-based Restaurant Recommendation System thumbnail" class="project-thumb">
            <!-- <h3 class="project-title">AI-based Restaurant Recommendation System</h3> -->
            <h3 class="project-title">
              <a href="pages/projects/project-recommendation.html">AI-based Restaurant Recommendation System</a>
            </h3>
            <p class="project-period">Jan. 2018 – Nov. 2018</p>
            <p class="project-desc">
              Built a restaurant recommendation system using Bi-LSTM with Word2Vec embeddings, achieving sentiment-based filtering with 70%+ positive reviews.
            </p>
          </div>
        </li>
    
      </ul>
    </section>


    <!-- Talks & Presentations -->
    <section id="talks">
      <h2>Talks & Presentations</h2>
      <ul class="timeline">
        <!-- 회사 발표 -->
        <li>
          <h3 class="timeline-header">
            Internal Seminar (Journal Club) | Samsung Research
          </h3>

          <div class="header-period">
            Feb. 2023 – Present
          </div>
          
          <div class="timeline-body">
            Presented papers:
            <ul>
              <li>"Addition is almost all you need: Compressing neural networks with double binary factorization", Aug. 2025</li>
              <li>"SmolVLM: Redefining small and efficient multimodal models", Apr. 2025</li>
              <li>"You Only Cache Once: Decoder-Decoder Architectures for Language Models", Nov. 2024</li>
              <li>"LoRA: Low-Rank Adaptation of Large Language Models", Jul. 2024</li>
              <li>"Introduction to LLM: From GPT to Chinchilla and LLaMA", Mar. 2024</li>
              <li>"PTQ4ViT: Post-Training Quantization for Vision Transformers with Twin Uniform Quantization", Dec. 2023</li>
              <li>"cosFormer: Rethinking Softmax In Attention", Jul. 2023</li>
              <li>"FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer", Feb. 2023</li>
            </ul>
          </div>
        </li>
    
        <!-- 석사 발표 -->
        <li>
          <h3 class="timeline-header">
            Graduate Seminar | KAIST
          </h3>
          <div class="header-period">
            Jan. 2019 – May. 2021
          </div>
          <div class="timeline-body">
            Presented papers:
            <ul>
              <li>"Knowledge Distillation and Beyond: From FitNet and Born-Again Networks to Noisy Time-Series Models", May 2021 [<a href="assets/presentations/noisy_timeseries.pdf" target="_blank">Slides</a>]</li>
              <li>"Knowledge Distillation in Time-series (2)", Nov. 2020 [<a href="assets/presentations/kd_timeseries2.pdf" target="_blank">Slides</a>]</li>
              <li>"Knowledge Distillation in Time-series (1)", Oct. 2020 [<a href="assets/presentations/kd_timeseries1.pdf" target="_blank">Slides</a>]</li>
              <li>"Model Compression and Acceleration", July 2020 [<a href="assets/presentations/model_compression.pdf" target="_blank">Slides</a>]</li>
              <li>"A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data", Feb. 2020 [<a href="assets/presentations/MSCRED.pdf" target="_blank">Slides</a>]</li>
              <li>"LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection", Jan. 2020 [<a href="assets/presentations/lstm_anomaly.pdf" target="_blank">Slides</a>]</li>
              <!-- 2021.05.01 발표 자료 정리 후 업데이트 예정 -->
            </ul>
          </div>
        </li>

      </ul>
    </section>

    <section id="honors">
      <h2>Honors & Scholarships</h2>
      <ul class="timeline">
        <li>
          <h3 class="timeline-header">
            Outstanding Teaching Assistant Award | <span>KAIST</span>
          </h3>
          <div class="header-period">
            Jun. 2020
          </div>
          <div class="timeline-body">
            Selected as an Outstanding Teaching Assistant for the 'Data Structures' course in recognition of receiving top-tier scores on end-of-semester student evaluations.
          </div>
        </li>
    
        <li>
          <h3 class="timeline-header">
            The Hongik Scholarship ($15,900 in total ≈ 4 semesters) | <span>Hongik University</span>
          </h3>
          <div class="header-period">
            Aug. 2015 – Sep. 2018
          </div>
          <div class="timeline-body">
            Continuously recognized for top academic achievements with a merit scholarship awarded annually. Over four years, I received a total of approximately $15,900, which included two full-tuition waivers for exceptional grades.
          </div>
        </li>
    
        <li>
          <h3 class="timeline-header">
            Korea Open Source Software Developers Lab (KOSS) Hackathon (2nd place) | <span>Korea IT Business Promotion Association</span>
          </h3>
          <div class="header-period">
            Oct. 2016
          </div>
          <div class="timeline-body">
            As a member of the Linux perf team, contributed to the open-source kernel tool by analyzing its source code and implementing a visualization feature for the srcline function to improve its usability.
          </div>
        </li>
    
      </ul>
    </section>

    <!-- Teaching Experience -->
    <section id="teaching">
      <h2>Teaching Assistant</h2>
      <div class="teaching-list">
        <h3 class="teaching-item">
          <span class="teaching-title">Database System</span>, Graduate Course |
          <span class="teaching-org">KAIST</span> |
          <span class="teaching-date">Mar. 2021 – Jun. 2021</span>
        </h3>
        <h3 class="teaching-item">
          <span class="teaching-title">System Programming</span>, Undergraduate Course |
          <span class="teaching-org">KAIST</span> |
          <span class="teaching-date">Sep. 2020 – Dec. 2020</span>
        </h3>
        <h3 class="teaching-item">
          <span class="teaching-title">Data Structure</span>, Undergraduate Course |
          <span class="teaching-org">KAIST</span> |
          <span class="teaching-date">Mar. 2020 – Jun. 2020</span>
        </h3>
      </div>
    </section>

    <!-- Extracurricular Activities -->
    <section id="extracurricular-activities">
      <h2>Extracurricular Activities</h2>
      <ul class="timeline">
    
        <li>
          <h3 class="timeline-header">
            Vice President, Graduate Student Association | <span>School of Computing, KAIST</span>
          </h3>
          <div class="header-period">
            Mar. 2020 – Feb. 2021
          </div>
          <div class="timeline-body">
            Took a leadership role in organizing departmental events and acted as a key liaison, fostering close communication and a supportive environment within the graduate student community.
          </div>
        </li>
    
        </ul>
    </section>

    </section>

    <!-- MISC -->
    <section id="misc">
      <h2>MISC</h2>
      
      <div class="misc-content">
        <p>
          I thrive on new experiences and believe in learning through them. 
          Outside the lab, I stay active with <strong>climbing, hiking, and running</strong>. 
          I also enjoy traveling, <strong>playing the piano</strong>, and reading books.
        </p>
      </div>

      <div class="misc-gallery">
        <figure class="misc-item">
          <img src="path/to/your-climbing-photo.jpg" alt="Rock Climbing">
          <figcaption>Pushing my limits on the wall</figcaption>
        </figure>

        <figure class="misc-item">
          <img src="path/to/your-travel-photo.jpg" alt="Travel to Ocean">
          <figcaption>Recharging with nature</figcaption>
        </figure>
      </div>
      
    </section>


  </main>
</body>
</html>
