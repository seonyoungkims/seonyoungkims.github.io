<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Project: Edge Computing–Based Anomaly Detection in Memory Semiconductor Processes</title>
  
  <link rel="stylesheet" href="../../styles.css">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
</head>
<body>
  <main class="container">

    <h1 class="page-title">Edge Computing–Based Anomaly Detection in Memory Semiconductor Processes</h1>

    <div classs="page-links">
      <a href="../../index.html#projects">← Back to Main</a>
    </div>

    <section class="project-detail">
      <img src="../../assets/images/Pr4.png" alt="Efficient AI Deployment on NPUs thumbnail" style="width:100%; max-width:600px; margin-bottom: 20px; display: block; margin-left: auto; margin-right: auto; border-radius: 12px;">
      <h2>Project Overview</h2>
      <h3>Motivation</h3>
      <p>
        Monitoring high-dimensional multivariate sensor data from semiconductor mass production using manual methods or simple thresholds had limitations in terms of accuracy and scalability. Therefore, this project aimed to develop an automated deep learning-based system for anomaly detection and Remaining Useful Life (RUL) prediction.
      </p>
      
      <h3>Achievements</h3>

      <li>
        Unsupervised Anomaly Detection
        <ul>
          <li>Designed a sequence reconstruction pipeline to detect anomalies based on reconstruction error.</li>
          <li>The model was trained to learn normal patterns using unlabeled real-world process data, enabling the detection of unseen anomaly types.</li>
        </ul>
      </li>

      <li>
        Architecture Search (Transformer vs. LSTM)
        <ul>
          <li>Conducted a comparative analysis between LSTM Autoencoder and Transformer models to address the long-term dependency problem in sensor time-series data.</li>
          <li>The Transformer architecture was ultimately adopted as it demonstrated superior performance in processing long sequences compared to the LSTM baseline.</li>
        </ul>
      </li>
      
      <li>
        RUL (Remaining Useful Life) Prediction
        <ul>
          <li>Implemented a Stacked LSTM Regression model to estimate the remaining time until equipment failure based on current process states.</li>
        </ul>
      </li>

    </section>

  </main>
</body>
</html>
