<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Project: Efficient AI Deployment on NPUs</title>
  
  <link rel="stylesheet" href="../../styles.css">
  
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
</head>
<body>
  <main class="container">

    <h1 class="page-title">Efficient AI Deployment on NPUs</h1>

    <div classs="page-links">
      <a href="../../index.html#projects">‚Üê Back to Main</a>
    </div>

    <section class="project-detail">
      <img src="../../assets/images/Pr6.png" alt="Efficient AI Deployment on NPUs thumbnail" style="width:100%; max-width:600px; margin-bottom: 20px; display: block; margin-left: auto; margin-right: auto; border-radius: 12px;">
      <h2>Project Highlights</h2>
    
      <h3>Bespoke LUT</h3>
      <ul>
        <li>Problem: Identified that Activation functions processed on DSPs (Digital Signal Processors) were the decoding bottleneck, based on profiling Samsung (in-house) and Qualcomm NPUs.</li>
        <li>Solution: Researched a technique to replace this DSP operation with a hardware-friendly Look-Up Table (LUT).</li>
        <li>Applied Models: Vision Models (ViT, DeiT, EfficientFormer, DETR) and LLMs (TinyLlama, SmolLM, OPT).</li>
        <li>Result: Achieved a 3.3x speedup in latency.</li>
      </ul>
    
      <h3>RaBiT</h3>
      <ul>
        <li>Problem: In existing binarization methods (1-bit 2-path), the residual path failed to perform effective error compensation.</li>
        <li>Solution: Optimized the residual path's error compensation by modifying the weight initialization and gradient calculation processes.</li>
        <li>Applied Models: LLMs (Llama2, Llama3, Gemma).</li>
        <li>Result: Attained SOTA performance on Llama and Gemma, with a 4.49x inference speedup.</li>
      </ul>
    
      <h3>Speech Model (ConformerT) NPU Deployment</h3>
      <ul>
        <li>Converted the model for NPU deployment and applied 8/16 mixed-precision quantization.</li>
        <li>Explored the optimal accuracy-performance (memory/speed) trade-off point using accuracy-based sensitivity analysis and profiling.</li>
        <li>Successfully deployed with a Real-Time Factor (RTF) below 1.0.</li>
      </ul>
    
      <h3>Next-Generation NPU R&D</h3>
      <ul>
        <li>Researched various model compression techniques for next-generation NPU-SW co-design.</li>
        <li>Trained SmolLM from scratch, then applied and analyzed advanced compression techniques like VPTQ and 1.58-bit quantization.</li>
        <li>Applied the YOCO (You Only Cache Once) technique to support long context.</li>
      </ul>
    
      <h3>Framework Development Support</h3>
      <ul>
        <li>
          <strong>QUEST (Role: Contributor)</strong>
          <ul>
            <li>Implemented, validated, and conducted unit tests for LUT and microscaling techniques in the in-house quantization tool.</li>
          </ul>
        </li>
        <li>
          <strong>LLMs (Role: Owner)</strong>
          <ul>
            <li>Developed a tool supporting the full pipeline from data preprocessing to quantization for various LLMs (Llama, Gemma, OPT, etc.).</li>
            <li>Used as the foundational framework for on-device LLM research.</li>
          </ul>
        </li>
      </ul>

    </section>

  </main>
</body>
</html>
